1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/188ffcee146a758d.css","style"]
0:{"P":null,"b":"YbSjjBiBTJ1PK7y8-5g5f","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/188ffcee146a758d.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Misc","type":"page","target":"misc","href":"/misc"}],"siteTitle":"Qi Cao","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"Jan 30, 2026"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","iNrH99UC7JWmHsJyLfLYK",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","588","static/chunks/588-d4a3dd10f217f95c.js","182","static/chunks/app/%5Bslug%5D/page-de362954567970d4.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
16:T4c7,Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.17:T64d,@article{cao2026scope,
  title = {Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning},
  author = {Qi Cao and Shuhao Zhang and Ruizhe Zhou and Ruiyi Zhang and Peijia Qin and Pengtao Xie},
  year = {2026},
  month = {jan},
  journal = {Arxiv Preprint},
  doi = {https://www.arxiv.org/abs/2601.22323},
  urldate = {2026-01-29},
  langid = {english},
  abstract = {Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.}
}18:T6ec,The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.19:T8bb,@article{wang2026generalfusion,
  title = {A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding},
  author = {Wu Wang and Liang-Jian Deng and Qi Cao and Gemine Vivone},
  year = {2026},
  month = {jan},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  doi = {https://ieeexplore.ieee.org/abstract/document/11359013},
  urldate = {2026-01-19},
  langid = {english},
  abstract = {The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.}
}1a:T51c,Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.1b:T67b,@article{qin2026daj,
  title = {DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation},
  author = {Peijia Qin and Ruiyi Zhang and Qi Cao and Pengtao Xie},
  year = {2026},
  month = {jan},
  journal = {Arxiv Preprint},
  doi = {https://www.arxiv.org/abs/2601.22230},
  urldate = {2026-01-29},
  langid = {english},
  abstract = {Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.}
}1c:T4f1,@article{zhang2025dreamprmcode,
  title = {DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding},
  author = {Ruiyi Zhang and Peijia Qin and Qi Cao and Pengtao Xie},
  year = {2025},
  month = {dec},
  journal = {Arxiv Preprint},
  doi = {https://arxiv.org/abs/2512.15000},
  urldate = {2025-12-17},
  langid = {english},
  abstract = {Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.}
}1d:T5ee,Training multimodal process reward models (PRMs) is hard due to (i) distribution shift between training set and test set and (ii) quality imbalance across training data samples. While domain-level reweighting (e.g., DreamPRM) aligns training with test-time objectives, it leaves a clear gap to an oracle upper bound (pass@N), even under a "sanity check" that uses test set data to probe headroom -- pointing to meta-level under-parameterization. We introduce DreamPRM-1.5, an instance-level reweighting framework that assigns an adaptive weight to every training example via bi-level optimization. To realize instance reweighting across scales, we develop two complementary regimes: Instance Table, which learns explicit per-sample weights and excels on small/medium data, and Instance Net, a lightweight neural network that generalizes better and scales to large corpora. A practical, stable training recipe -- time-scale matching between upper/lower updates, cold-start initialization, and bounded-range weights -- prevents divergence. Integrated with test-time scaling, DreamPRM-1.5 attains 84.6 accuracy on the MMMU validation set, 31.3 accuracy on R-Bench-V and, when paired with a leading backbone (e.g., GPT-5-mini), achieves first-place results on public multimodal reasoning leaderboards. Moreover, extensive experiments, including benchmark evaluations, baseline comparisons, and a sanity check, demonstrate that DreamPRM-1.5 closes the gap toward the oracle, achieves leading performance, and trains stably.1e:T74d,@article{cao2025dreamprm15,
  title = {DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training},
  author = {Qi Cao and Pengtao Xie},
  year = {2025},
  month = {sep},
  journal = {Arxiv Preprint},
  doi = {https://arxiv.org/abs/2509.05542},
  urldate = {2025-10-21},
  langid = {english},
  abstract = {Training multimodal process reward models (PRMs) is hard due to (i) distribution shift between training set and test set and (ii) quality imbalance across training data samples. While domain-level reweighting (e.g., DreamPRM) aligns training with test-time objectives, it leaves a clear gap to an oracle upper bound (pass@N), even under a "sanity check" that uses test set data to probe headroom -- pointing to meta-level under-parameterization. We introduce DreamPRM-1.5, an instance-level reweighting framework that assigns an adaptive weight to every training example via bi-level optimization. To realize instance reweighting across scales, we develop two complementary regimes: Instance Table, which learns explicit per-sample weights and excels on small/medium data, and Instance Net, a lightweight neural network that generalizes better and scales to large corpora. A practical, stable training recipe -- time-scale matching between upper/lower updates, cold-start initialization, and bounded-range weights -- prevents divergence. Integrated with test-time scaling, DreamPRM-1.5 attains 84.6 accuracy on the MMMU validation set, 31.3 accuracy on R-Bench-V and, when paired with a leading backbone (e.g., GPT-5-mini), achieves first-place results on public multimodal reasoning leaderboards. Moreover, extensive experiments, including benchmark evaluations, baseline comparisons, and a sanity check, demonstrate that DreamPRM-1.5 closes the gap toward the oracle, achieves leading performance, and trains stably.}
}1f:T4f9,Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (eg, DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and fewshot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4% on average across sentiment and natural language inference tasks, including gains of 7.3% on the Mental Health dataset and 10.9% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation. Our code is available at https://github.com/coder-qicao/RL4GLUE.20:T711,@inproceedings{somayajula2025improving,
  title = {Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning},
  author = {Sai Ashish Somayajula and Bokai Hu and Qi Cao and Xin Pan and Pengtao Xie},
  year = {2025},
  month = {jun},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP},
  doi = {https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2025.findings-emnlp.1392.pdf},
  urldate = {2025-04-04},
  langid = {english},
  abstract = {Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (eg, DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and fewshot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4% on average across sentiment and natural language inference tasks, including gains of 7.3% on the Mental Health dataset and 10.9% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation. Our code is available at https://github.com/coder-qicao/RL4GLUE.}
}21:T762,Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.22:T92d,@inproceedings{cao2025dreamprm,
  title = {DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning},
  author = {Qi Cao and Ruiyi Wang and Ruiyi Zhang and Sai Ashish Somayajula and Pengtao Xie},
  year = {2025},
  month = {may},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  doi = {https://arxiv.org/abs/2505.20241},
  urldate = {2025-11-04},
  langid = {english},
  abstract = {Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.}
}23:T51e,Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization.24:T68b,@article{shan2025mint,
  title = {MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping},
  author = {Xiaojun Shan and Qi Cao and Xing Han and Haofei Yu and Paul Pu Liang},
  year = {2025},
  month = {may},
  journal = {Arxiv Preprint},
  doi = {https://arxiv.org/abs/2506.02308},
  urldate = {2025-06-02},
  langid = {english},
  abstract = {Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization.}
}25:T5c2,Pansharpening is a challenging low-level vision task whose aim is to learn the complementary representation between spectral information and spatial detail. Despite the remarkable progress, existing deep neural network (DNN)-based pansharpening algorithms are still confronted with common limitations. These methods rarely consider the local specificity of different spectral bands; They often extract global details in the spatial domain while ignoring task-related degradation, e.g., the down-sampling process of multispectral (MS) images, and thus suffer from limited receptive fields. In this work, we propose a novel bidomain modeling paradigm for the pansharpening problem, dubbed BiPan, which takes both local spectral specificity and global spatial detail into account. More specifically, we first customize the specialized source-discriminative adaptive convolution (SDAC) for every spectral band instead of sharing kernels across all bands as in prior works. Then, we devise a novel Fourier global modeling module (FGMM), which is capable of embracing global information while benefiting from the disentanglement of image degradation. By integrating the band-aware local feature and Fourier global detail from these two functional designs, we can fuse a texture-rich while visually pleasing high-resolution MS image. Extensive experiments demonstrate that the proposed framework achieves favorable performance against current state-of-the-art pansharpening methods26:T778,@inproceedings{cao2023bimpan,
  title = {Bidomain Modeling Paradigm for Pansharpening},
  author = {Junming Hou and Qi Cao and Ran Ran and Che Liu and Junling Li and Liang-jian Deng},
  year = {2024},
  month = {oct},
  booktitle = {Proceedings of the 31st ACM international conference on multimedia (ACM MM)},
  doi = {https://dl.acm.org/doi/pdf/10.1145/3581783.3612188},
  urldate = {2023-10-26},
  langid = {english},
  abstract = {Pansharpening is a challenging low-level vision task whose aim is to learn the complementary representation between spectral information and spatial detail. Despite the remarkable progress, existing deep neural network (DNN)-based pansharpening algorithms are still confronted with common limitations. These methods rarely consider the local specificity of different spectral bands; They often extract global details in the spatial domain while ignoring task-related degradation, e.g., the down-sampling process of multispectral (MS) images, and thus suffer from limited receptive fields. In this work, we propose a novel bidomain modeling paradigm for the pansharpening problem, dubbed BiPan, which takes both local spectral specificity and global spatial detail into account. More specifically, we first customize the specialized source-discriminative adaptive convolution (SDAC) for every spectral band instead of sharing kernels across all bands as in prior works. Then, we devise a novel Fourier global modeling module (FGMM), which is capable of embracing global information while benefiting from the disentanglement of image degradation. By integrating the band-aware local feature and Fourier global detail from these two functional designs, we can fuse a texture-rich while visually pleasing high-resolution MS image. Extensive experiments demonstrate that the proposed framework achieves favorable performance against current state-of-the-art pansharpening methods}
}27:T7be,Pansharpening refers to fusing a low-resolution multispectral image (LRMS) and a high-resolution panchromatic (PAN) image to generate a high-resolution multispectral image (HRMS). Traditional pansharpening methods use a single pair of LRMS and PAN to generate HRMS at full resolution, but they fail to generate high-quality fused products due to the assumption of a (often inaccurate) linear relationship between the fused products. Convolutional neural network methods, i.e., supervised and unsupervised learning approaches, can model any arbitrary non-linear relationship among data, but performing even worse than traditional methods when testing data are not consistent with training data. Moreover, supervised methods rely on simulating reduced resolution data for training causing information loss at full resolution. Unsupervised pansharpening suffers from distortion due to the lack of reference images and inaccuracy in the estimation of the degradation process. In this paper, we propose a zero-shot semi-supervised method for pansharpening (ZS-Pan), which only requires a single pair of PAN/LRMS images for training and testing networks combining both the pros of supervised and unsupervised methods. Facing with challenges of limited training data and no reference images, the ZS-Pan framework is built with a two-phase three-component model, i.e., the reduced resolution supervised pre-training (RSP), the spatial degradation establishment (SDE), and the full resolution unsupervised generation (FUG) stages. Specifically, a special parameter initialization technique, a data augmentation strategy, and a non-linear degradation network are proposed to improve the representation ability of the network. In our experiments, we evaluate the performance of the proposed framework on different datasets using some state-of-the-art (SOTA) pansharpening approaches for comparison. Results show that our ZS-Pan outperforms these SOTA methods, both visually and quantitatively.28:T9a9,@article{cao2024zspan,
  title = {Zero-shot Semi-supervised Learning for Pansharpening},
  author = {Qi Cao and Liang-Jian Deng and Wu Wang and Junming Hou and Gemine Vivone},
  year = {2024},
  month = {jan},
  journal = {Information Fusion},
  doi = {https://www.sciencedirect.com/science/article/pii/S1566253523003172?casa_token=UoG5K9i4aJcAAAAA:d3EaM7LaU-RuamWNhH9g0IkFl_MOGsxi46uNlffI6a4JoWw8NkYF4DKk_UtCLUSYjASQWXQIUg},
  urldate = {2024-01-01},
  langid = {english},
  abstract = {Pansharpening refers to fusing a low-resolution multispectral image (LRMS) and a high-resolution panchromatic (PAN) image to generate a high-resolution multispectral image (HRMS). Traditional pansharpening methods use a single pair of LRMS and PAN to generate HRMS at full resolution, but they fail to generate high-quality fused products due to the assumption of a (often inaccurate) linear relationship between the fused products. Convolutional neural network methods, i.e., supervised and unsupervised learning approaches, can model any arbitrary non-linear relationship among data, but performing even worse than traditional methods when testing data are not consistent with training data. Moreover, supervised methods rely on simulating reduced resolution data for training causing information loss at full resolution. Unsupervised pansharpening suffers from distortion due to the lack of reference images and inaccuracy in the estimation of the degradation process. In this paper, we propose a zero-shot semi-supervised method for pansharpening (ZS-Pan), which only requires a single pair of PAN/LRMS images for training and testing networks combining both the pros of supervised and unsupervised methods. Facing with challenges of limited training data and no reference images, the ZS-Pan framework is built with a two-phase three-component model, i.e., the reduced resolution supervised pre-training (RSP), the spatial degradation establishment (SDE), and the full resolution unsupervised generation (FUG) stages. Specifically, a special parameter initialization technique, a data augmentation strategy, and a non-linear degradation network are proposed to improve the representation ability of the network. In our experiments, we evaluate the performance of the proposed framework on different datasets using some state-of-the-art (SOTA) pansharpening approaches for comparison. Results show that our ZS-Pan outperforms these SOTA methods, both visually and quantitatively.}
}7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"cao2026scope","title":"Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Shuhao Zhang","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruizhe Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peijia Qin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"month":"1","type":"journal","status":"published","tags":["Model Routing","LLM Reasoning","Reinforcement Learning","Retrieval Augmentation"],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://www.arxiv.org/abs/2601.22323","code":"https://github.com/Sullivan07043/SCOPE-Router","abstract":"$16","description":"SCOPE, a model routing framework that predicts how accurate and how expensive each model will be before running it, allowing users to control cost-accuracy trade-offs and naturally handle new models.","selected":true,"bibtex":"$17"},{"id":"wang2026generalfusion","title":"A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding","authors":[{"name":"Wu Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liang-Jian Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Gemine Vivone","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"month":"1","type":"journal","status":"published","tags":["Image Fusion","Transfer Learning"],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)","conference":"","doi":"https://ieeexplore.ieee.org/abstract/document/11359013","abstract":"$18","description":"We propose a unified deep image fusion framework that uses sequential gradient-transfer training and fusion-rule unfolding in a deep equilibrium network to efficiently handle multiple fusion tasks and generalize strongly to unseen ones (e.g., medical fusion).","selected":false,"bibtex":"$19"},{"id":"qin2026daj","title":"DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation","authors":[{"name":"Peijia Qin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"month":"1","type":"journal","status":"published","tags":["LLM as a Judge","Code Generation","Bi-level Optimization"],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://www.arxiv.org/abs/2601.22230","abstract":"$1a","description":"We introduce DAJ, the first data-reweighted, reasoning-based LLM judge for Best-of-N test-time scaling, trained with verifiable rewards to address distribution shift and achieve state-of-the-art performance on LiveCodeBench and BigCodeBench.","selected":false,"bibtex":"$1b"},{"id":"zhang2025dreamprmcode","title":"DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding","authors":[{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peijia Qin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"12","type":"journal","status":"published","tags":["Process Reward Model","Code Generation","Bi-level Optimization"],"keywords":"$7:props:children:0:props:publications:3:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://arxiv.org/abs/2512.15000","abstract":"Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.","description":"Chain-of-Function and domain-reweighted Coding PRM training.","selected":false,"bibtex":"$1c"},{"id":"cao2025dreamprm15","title":"DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"9","type":"journal","status":"published","tags":["Process Reward Model","LLM Reasoning","Bi-level Optimization"],"keywords":"$7:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://arxiv.org/abs/2509.05542","code":"https://github.com/coder-qicao/DreamPRM-1.5","abstract":"$1d","description":"An instance-reweighting updated version of DreamPRM, higher accuracy and more robust.","selected":false,"bibtex":"$1e"},{"id":"somayajula2025improving","title":"Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning","authors":[{"name":"Sai Ashish Somayajula","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Bokai Hu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Xin Pan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"6","type":"conference","status":"published","tags":["Process Reward Model","LLM Reasoning","Bi-level Optimization"],"keywords":"$7:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"Findings of the Association for Computational Linguistics: EMNLP","doi":"https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2025.findings-emnlp.1392.pdf","code":"https://github.com/coder-qicao/RL4GLUE","abstract":"$1f","description":"Using PPO to reframe NLU as token-level reinforcement learning with label-based rewards, we substantially boost <14B instruction-tuned LLMs on GLUE/SuperGLUE—beating supervised fine-tuning and even GPT-4o on several sentiment and NLI datasets.","selected":false,"bibtex":"$20"},{"id":"cao2025dreamprm","title":"DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sai Ashish Somayajula","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":["Process Reward Model","LLM Reasoning","Bi-level Optimization"],"keywords":"$7:props:children:0:props:publications:6:tags","researchArea":"machine-learning","journal":"","conference":"The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)","doi":"https://arxiv.org/abs/2505.20241","code":"https://github.com/coder-qicao/DreamPRM","abstract":"$21","description":"A multimodal Process Reward Model (PRM) trained with domain-reweighting. Top 1 method on MathVista, MMMU & R-Bench-V.","selected":true,"awards":["Spotlight @ Multimodal Algorithmic Reasoning Workshop"],"bibtex":"$22"},{"id":"shan2025mint","title":"MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping","authors":[{"name":"Xiaojun Shan","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Xing Han","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Haofei Yu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Paul Pu Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"journal","status":"published","tags":["Multimodal Interaction","Instruction Tuning"],"keywords":"$7:props:children:0:props:publications:7:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://arxiv.org/abs/2506.02308","abstract":"$23","description":"Adding more multimodal instruction-tuning tasks alone is unreliable—MINT groups tasks by modality interaction (redundancy, selection, fusion) to reduce interference and improve generalization–specialization performance.","selected":false,"bibtex":"$24"},{"id":"cao2023bimpan","title":"Bidomain Modeling Paradigm for Pansharpening","authors":[{"name":"Junming Hou","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Ran Ran","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Che Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junling Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liang-jian Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"10","type":"conference","status":"published","tags":["Pansharpening","Adaptive Convolution","Fourier Convolution"],"keywords":"$7:props:children:0:props:publications:8:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 31st ACM international conference on multimedia (ACM MM)","doi":"https://dl.acm.org/doi/pdf/10.1145/3581783.3612188","code":"https://github.com/coder-qicao/BiMPan","abstract":"$25","description":"We propose BiPan, a bidomain pansharpening framework that models band-specific local spectral features and global spatial details in the Fourier domain, achieving state-of-the-art performance by better handling spectral diversity and MS image degradation.","selected":false,"awards":["Oral"],"bibtex":"$26"},{"id":"cao2024zspan","title":"Zero-shot Semi-supervised Learning for Pansharpening","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Liang-Jian Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wu Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junming Hou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Gemine Vivone","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"1","type":"journal","status":"published","tags":["Pansharpening","Zero-shot Learning","Semi-supervised Learning"],"keywords":"$7:props:children:0:props:publications:9:tags","researchArea":"machine-learning","journal":"Information Fusion","conference":"","doi":"https://www.sciencedirect.com/science/article/pii/S1566253523003172?casa_token=UoG5K9i4aJcAAAAA:d3EaM7LaU-RuamWNhH9g0IkFl_MOGsxi46uNlffI6a4JoWw8NkYF4DKk_UtCLUSYjASQWXQIUg","code":"https://github.com/coder-qicao/ZS-Pan","abstract":"$27","description":"Zero-shot pansharpening (ZS-Pan) only requires a single pair of PAN/LRMS images. Any pansharpening network can take the ZS-Pan as a plug-and-play module. A two-phase three-component semi-supervised model is designed for ZS-Pan.","selected":true,"bibtex":"$28"}]}],false,false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Qi Cao"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Qi Cao"}],["$","meta","3",{"name":"keywords","content":"Qi Cao,PhD,Research,University of California, San Diego"}],["$","meta","4",{"name":"creator","content":"Qi Cao"}],["$","meta","5",{"name":"publisher","content":"Qi Cao"}],["$","meta","6",{"property":"og:title","content":"Qi Cao"}],["$","meta","7",{"property":"og:description","content":"PhD student at the University of California, San Diego."}],["$","meta","8",{"property":"og:site_name","content":"Qi Cao's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Qi Cao"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at the University of California, San Diego."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
