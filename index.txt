1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-a2d7e4275da50bac.js"],"default"]
7:I[7437,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","588","static/chunks/588-a5828d931fc670dd.js","974","static/chunks/app/page-3eccc35055b7dce4.js"],"default"]
8:I[9507,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","588","static/chunks/588-a5828d931fc670dd.js","974","static/chunks/app/page-3eccc35055b7dce4.js"],"default"]
9:I[1990,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","588","static/chunks/588-a5828d931fc670dd.js","974","static/chunks/app/page-3eccc35055b7dce4.js"],"default"]
a:I[5218,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","588","static/chunks/588-a5828d931fc670dd.js","974","static/chunks/app/page-3eccc35055b7dce4.js"],"default"]
15:I[9665,[],"MetadataBoundary"]
17:I[9665,[],"OutletBoundary"]
1a:I[4911,[],"AsyncMetadataOutlet"]
1c:I[9665,[],"ViewportBoundary"]
1e:I[6614,[],""]
:HL["/_next/static/css/188ffcee146a758d.css","style"]
b:T4c7,Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.c:T64d,@article{cao2026scope,
  title = {Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning},
  author = {Qi Cao and Shuhao Zhang and Ruizhe Zhou and Ruiyi Zhang and Peijia Qin and Pengtao Xie},
  year = {2026},
  month = {jan},
  journal = {Arxiv Preprint},
  doi = {https://www.arxiv.org/abs/2601.22323},
  urldate = {2026-01-29},
  langid = {english},
  abstract = {Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.}
}d:T5ee,Training multimodal process reward models (PRMs) is hard due to (i) distribution shift between training set and test set and (ii) quality imbalance across training data samples. While domain-level reweighting (e.g., DreamPRM) aligns training with test-time objectives, it leaves a clear gap to an oracle upper bound (pass@N), even under a "sanity check" that uses test set data to probe headroom -- pointing to meta-level under-parameterization. We introduce DreamPRM-1.5, an instance-level reweighting framework that assigns an adaptive weight to every training example via bi-level optimization. To realize instance reweighting across scales, we develop two complementary regimes: Instance Table, which learns explicit per-sample weights and excels on small/medium data, and Instance Net, a lightweight neural network that generalizes better and scales to large corpora. A practical, stable training recipe -- time-scale matching between upper/lower updates, cold-start initialization, and bounded-range weights -- prevents divergence. Integrated with test-time scaling, DreamPRM-1.5 attains 84.6 accuracy on the MMMU validation set, 31.3 accuracy on R-Bench-V and, when paired with a leading backbone (e.g., GPT-5-mini), achieves first-place results on public multimodal reasoning leaderboards. Moreover, extensive experiments, including benchmark evaluations, baseline comparisons, and a sanity check, demonstrate that DreamPRM-1.5 closes the gap toward the oracle, achieves leading performance, and trains stably.e:T74d,@article{cao2025dreamprm15,
  title = {DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training},
  author = {Qi Cao and Pengtao Xie},
  year = {2025},
  month = {sep},
  journal = {Arxiv Preprint},
  doi = {https://arxiv.org/abs/2509.05542},
  urldate = {2025-10-21},
  langid = {english},
  abstract = {Training multimodal process reward models (PRMs) is hard due to (i) distribution shift between training set and test set and (ii) quality imbalance across training data samples. While domain-level reweighting (e.g., DreamPRM) aligns training with test-time objectives, it leaves a clear gap to an oracle upper bound (pass@N), even under a "sanity check" that uses test set data to probe headroom -- pointing to meta-level under-parameterization. We introduce DreamPRM-1.5, an instance-level reweighting framework that assigns an adaptive weight to every training example via bi-level optimization. To realize instance reweighting across scales, we develop two complementary regimes: Instance Table, which learns explicit per-sample weights and excels on small/medium data, and Instance Net, a lightweight neural network that generalizes better and scales to large corpora. A practical, stable training recipe -- time-scale matching between upper/lower updates, cold-start initialization, and bounded-range weights -- prevents divergence. Integrated with test-time scaling, DreamPRM-1.5 attains 84.6 accuracy on the MMMU validation set, 31.3 accuracy on R-Bench-V and, when paired with a leading backbone (e.g., GPT-5-mini), achieves first-place results on public multimodal reasoning leaderboards. Moreover, extensive experiments, including benchmark evaluations, baseline comparisons, and a sanity check, demonstrate that DreamPRM-1.5 closes the gap toward the oracle, achieves leading performance, and trains stably.}
}f:T762,Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.10:T92d,@inproceedings{cao2025dreamprm,
  title = {DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning},
  author = {Qi Cao and Ruiyi Wang and Ruiyi Zhang and Sai Ashish Somayajula and Pengtao Xie},
  year = {2025},
  month = {may},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  doi = {https://arxiv.org/abs/2505.20241},
  urldate = {2025-11-04},
  langid = {english},
  abstract = {Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.}
}11:T5c2,Pansharpening is a challenging low-level vision task whose aim is to learn the complementary representation between spectral information and spatial detail. Despite the remarkable progress, existing deep neural network (DNN)-based pansharpening algorithms are still confronted with common limitations. These methods rarely consider the local specificity of different spectral bands; They often extract global details in the spatial domain while ignoring task-related degradation, e.g., the down-sampling process of multispectral (MS) images, and thus suffer from limited receptive fields. In this work, we propose a novel bidomain modeling paradigm for the pansharpening problem, dubbed BiPan, which takes both local spectral specificity and global spatial detail into account. More specifically, we first customize the specialized source-discriminative adaptive convolution (SDAC) for every spectral band instead of sharing kernels across all bands as in prior works. Then, we devise a novel Fourier global modeling module (FGMM), which is capable of embracing global information while benefiting from the disentanglement of image degradation. By integrating the band-aware local feature and Fourier global detail from these two functional designs, we can fuse a texture-rich while visually pleasing high-resolution MS image. Extensive experiments demonstrate that the proposed framework achieves favorable performance against current state-of-the-art pansharpening methods12:T778,@inproceedings{cao2023bimpan,
  title = {Bidomain Modeling Paradigm for Pansharpening},
  author = {Junming Hou and Qi Cao and Ran Ran and Che Liu and Junling Li and Liang-jian Deng},
  year = {2024},
  month = {oct},
  booktitle = {Proceedings of the 31st ACM international conference on multimedia (ACM MM)},
  doi = {https://dl.acm.org/doi/pdf/10.1145/3581783.3612188},
  urldate = {2023-10-26},
  langid = {english},
  abstract = {Pansharpening is a challenging low-level vision task whose aim is to learn the complementary representation between spectral information and spatial detail. Despite the remarkable progress, existing deep neural network (DNN)-based pansharpening algorithms are still confronted with common limitations. These methods rarely consider the local specificity of different spectral bands; They often extract global details in the spatial domain while ignoring task-related degradation, e.g., the down-sampling process of multispectral (MS) images, and thus suffer from limited receptive fields. In this work, we propose a novel bidomain modeling paradigm for the pansharpening problem, dubbed BiPan, which takes both local spectral specificity and global spatial detail into account. More specifically, we first customize the specialized source-discriminative adaptive convolution (SDAC) for every spectral band instead of sharing kernels across all bands as in prior works. Then, we devise a novel Fourier global modeling module (FGMM), which is capable of embracing global information while benefiting from the disentanglement of image degradation. By integrating the band-aware local feature and Fourier global detail from these two functional designs, we can fuse a texture-rich while visually pleasing high-resolution MS image. Extensive experiments demonstrate that the proposed framework achieves favorable performance against current state-of-the-art pansharpening methods}
}13:T7be,Pansharpening refers to fusing a low-resolution multispectral image (LRMS) and a high-resolution panchromatic (PAN) image to generate a high-resolution multispectral image (HRMS). Traditional pansharpening methods use a single pair of LRMS and PAN to generate HRMS at full resolution, but they fail to generate high-quality fused products due to the assumption of a (often inaccurate) linear relationship between the fused products. Convolutional neural network methods, i.e., supervised and unsupervised learning approaches, can model any arbitrary non-linear relationship among data, but performing even worse than traditional methods when testing data are not consistent with training data. Moreover, supervised methods rely on simulating reduced resolution data for training causing information loss at full resolution. Unsupervised pansharpening suffers from distortion due to the lack of reference images and inaccuracy in the estimation of the degradation process. In this paper, we propose a zero-shot semi-supervised method for pansharpening (ZS-Pan), which only requires a single pair of PAN/LRMS images for training and testing networks combining both the pros of supervised and unsupervised methods. Facing with challenges of limited training data and no reference images, the ZS-Pan framework is built with a two-phase three-component model, i.e., the reduced resolution supervised pre-training (RSP), the spatial degradation establishment (SDE), and the full resolution unsupervised generation (FUG) stages. Specifically, a special parameter initialization technique, a data augmentation strategy, and a non-linear degradation network are proposed to improve the representation ability of the network. In our experiments, we evaluate the performance of the proposed framework on different datasets using some state-of-the-art (SOTA) pansharpening approaches for comparison. Results show that our ZS-Pan outperforms these SOTA methods, both visually and quantitatively.14:T9a9,@article{cao2024zspan,
  title = {Zero-shot Semi-supervised Learning for Pansharpening},
  author = {Qi Cao and Liang-Jian Deng and Wu Wang and Junming Hou and Gemine Vivone},
  year = {2024},
  month = {jan},
  journal = {Information Fusion},
  doi = {https://www.sciencedirect.com/science/article/pii/S1566253523003172?casa_token=UoG5K9i4aJcAAAAA:d3EaM7LaU-RuamWNhH9g0IkFl_MOGsxi46uNlffI6a4JoWw8NkYF4DKk_UtCLUSYjASQWXQIUg},
  urldate = {2024-01-01},
  langid = {english},
  abstract = {Pansharpening refers to fusing a low-resolution multispectral image (LRMS) and a high-resolution panchromatic (PAN) image to generate a high-resolution multispectral image (HRMS). Traditional pansharpening methods use a single pair of LRMS and PAN to generate HRMS at full resolution, but they fail to generate high-quality fused products due to the assumption of a (often inaccurate) linear relationship between the fused products. Convolutional neural network methods, i.e., supervised and unsupervised learning approaches, can model any arbitrary non-linear relationship among data, but performing even worse than traditional methods when testing data are not consistent with training data. Moreover, supervised methods rely on simulating reduced resolution data for training causing information loss at full resolution. Unsupervised pansharpening suffers from distortion due to the lack of reference images and inaccuracy in the estimation of the degradation process. In this paper, we propose a zero-shot semi-supervised method for pansharpening (ZS-Pan), which only requires a single pair of PAN/LRMS images for training and testing networks combining both the pros of supervised and unsupervised methods. Facing with challenges of limited training data and no reference images, the ZS-Pan framework is built with a two-phase three-component model, i.e., the reduced resolution supervised pre-training (RSP), the spatial degradation establishment (SDE), and the full resolution unsupervised generation (FUG) stages. Specifically, a special parameter initialization technique, a data augmentation strategy, and a non-linear degradation network are proposed to improve the representation ability of the network. In our experiments, we evaluate the performance of the proposed framework on different datasets using some state-of-the-art (SOTA) pansharpening approaches for comparison. Results show that our ZS-Pan outperforms these SOTA methods, both visually and quantitatively.}
}0:{"P":null,"b":"zZo57skrtaNo5U6bMgYsw","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/188ffcee146a758d.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Misc","type":"page","target":"misc","href":"/misc"}],"siteTitle":"Qi Cao","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"Jan 30, 2026"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Qi Cao","title":"PhD Student","institution":"University of California, San Diego","avatar":"/bio.jpg"},"social":{"email":"q9cao@ucsd.edu","location":"La Jolla, CA","location_url":"https://maps.google.com","location_details":["3195 Voigt Dr,","La Jolla, CA 92093, US"],"google_scholar":"https://scholar.google.com/citations?user=46U_bhgAAAAJ&hl=en","github":"https://github.com/coder-qicao","linkedin":"https://www.linkedin.com/in/qi-cao-86a81b32a/"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["LLM Reasoning","Machine Learning","Reinforcement Learning"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"I am Qi Cao, a 2nd-year Ph.D. student in the Department of Electrical and Computer Engineering at the **University of California, San Diego**, advised by Prof. [Pengtao Xie](https://pengtaoxie.github.io/).\n\nPreviously, I received my B.S. in **Mathematics and Physics** Class from the **Yingcai Honors School** at the **University of Electronic Science and Technology of China**, where I was fortunate to be advised by Prof. [Liangjian Deng](https://liangjiandeng.github.io/index.html).\n\nI will join **Meta** as a research scientist intern in Summer 2026. My current research focuses on **large language model (LLM) reasoning** and **building LLM-based reasoning systems**.","title":"About"}],["$","$L9","news",{"items":[{"date":"2026-02","content":"We built a [project page](https://sullivan07043.github.io/SCOPE/) for SCOPE."},{"date":"2025-12","content":"I will join Meta as a research scientist intern in Summer 2026!"},{"date":"2024-09","content":"Starting my PhD at UCSD."}],"title":"News"}],["$","$La","featured_publications",{"publications":[{"id":"cao2026scope","title":"Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Shuhao Zhang","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Ruizhe Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peijia Qin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"month":"1","type":"journal","status":"published","tags":["Model Routing","LLM Reasoning","Reinforcement Learning","Retrieval Augmentation"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://www.arxiv.org/abs/2601.22323","code":"https://github.com/Sullivan07043/SCOPE-Router","website":"https://sullivan07043.github.io/SCOPE/","abstract":"$b","description":"SCOPE, a model routing framework that predicts how accurate and how expensive each model will be before running it, allowing users to control cost-accuracy trade-offs and naturally handle new models.","selected":true,"bibtex":"$c"},{"id":"cao2025dreamprm15","title":"DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"9","type":"journal","status":"published","tags":["Process Reward Model","LLM Reasoning","Bi-level Optimization"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags","researchArea":"machine-learning","journal":"Arxiv Preprint","conference":"","doi":"https://arxiv.org/abs/2509.05542","code":"https://github.com/coder-qicao/DreamPRM-1.5","abstract":"$d","description":"An instance-reweighting updated version of DreamPRM, higher accuracy and more robust.","selected":true,"bibtex":"$e"},{"id":"cao2025dreamprm","title":"DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruiyi Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sai Ashish Somayajula","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Pengtao Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":["Process Reward Model","LLM Reasoning","Bi-level Optimization"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)","doi":"https://arxiv.org/abs/2505.20241","code":"https://github.com/coder-qicao/DreamPRM","abstract":"$f","description":"A multimodal Process Reward Model (PRM) trained with domain-reweighting. Top 1 method on MathVista, MMMU & R-Bench-V.","selected":true,"awards":["Spotlight @ Multimodal Algorithmic Reasoning Workshop"],"bibtex":"$10"},{"id":"cao2023bimpan","title":"Bidomain Modeling Paradigm for Pansharpening","authors":[{"name":"Junming Hou","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Qi Cao","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Ran Ran","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Che Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junling Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Liang-jian Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"10","type":"conference","status":"published","tags":["Pansharpening","Adaptive Convolution","Fourier Convolution"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 31st ACM international conference on multimedia (ACM MM)","doi":"https://dl.acm.org/doi/pdf/10.1145/3581783.3612188","code":"https://github.com/coder-qicao/BiMPan","abstract":"$11","description":"We propose BiPan, a bidomain pansharpening framework that models band-specific local spectral features and global spatial details in the Fourier domain, achieving state-of-the-art performance by better handling spectral diversity and MS image degradation.","selected":true,"awards":["Oral"],"bibtex":"$12"},{"id":"cao2024zspan","title":"Zero-shot Semi-supervised Learning for Pansharpening","authors":[{"name":"Qi Cao","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Liang-Jian Deng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wu Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junming Hou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Gemine Vivone","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"1","type":"journal","status":"published","tags":["Pansharpening","Zero-shot Learning","Semi-supervised Learning"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:4:tags","researchArea":"machine-learning","journal":"Information Fusion","conference":"","doi":"https://www.sciencedirect.com/science/article/pii/S1566253523003172?casa_token=UoG5K9i4aJcAAAAA:d3EaM7LaU-RuamWNhH9g0IkFl_MOGsxi46uNlffI6a4JoWw8NkYF4DKk_UtCLUSYjASQWXQIUg","code":"https://github.com/coder-qicao/ZS-Pan","abstract":"$13","description":"Zero-shot pansharpening (ZS-Pan) only requires a single pair of PAN/LRMS images. Any pansharpening network can take the ZS-Pan as a plug-and-play module. A two-phase three-component semi-supervised model is designed for ZS-Pan.","selected":true,"bibtex":"$14"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false,false]}]]}]]}]}],["$","$L15",null,{"children":"$L16"}],null,["$","$L17",null,{"children":["$L18","$L19",["$","$L1a",null,{"promise":"$@1b"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","dYbzT6fq1LQ6ZJnpndrGH",{"children":[["$","$L1c",null,{"children":"$L1d"}],null]}],null]}],false]],"m":"$undefined","G":["$1e","$undefined"],"s":false,"S":true}
1f:"$Sreact.suspense"
20:I[4911,[],"AsyncMetadata"]
16:["$","$1f",null,{"fallback":null,"children":["$","$L20",null,{"promise":"$@21"}]}]
19:null
1d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
18:null
21:{"metadata":[["$","title","0",{"children":"Qi Cao"}],["$","meta","1",{"name":"description","content":"PhD student at the University of California, San Diego."}],["$","meta","2",{"name":"author","content":"Qi Cao"}],["$","meta","3",{"name":"keywords","content":"Qi Cao,PhD,Research,University of California, San Diego"}],["$","meta","4",{"name":"creator","content":"Qi Cao"}],["$","meta","5",{"name":"publisher","content":"Qi Cao"}],["$","meta","6",{"property":"og:title","content":"Qi Cao"}],["$","meta","7",{"property":"og:description","content":"PhD student at the University of California, San Diego."}],["$","meta","8",{"property":"og:site_name","content":"Qi Cao's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Qi Cao"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at the University of California, San Diego."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
1b:{"metadata":"$21:metadata","error":null,"digest":"$undefined"}
